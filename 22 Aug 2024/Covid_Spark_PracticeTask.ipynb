{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a21bad6-f3bf-453f-a7ae-7d6209e20bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Covid Data Analysis\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1a743a1-32af-4cb8-bcbd-6f37952d73e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file into a DataFrame\n",
    "df = spark.read.format(\"csv\") \\\n",
    "            .option(\"header\", True) \\\n",
    "            .option(\"multiLine\", True) \\\n",
    "            .option(\"ignoreLeadingWhiteSpace\",True) \\\n",
    "            .option(\"ignoreTrailingWhiteSpace\",True) \\\n",
    "            .option(\"escape\", \"\\\\\") \\\n",
    "            .option(\"quote\", \"\\\"\") \\\n",
    "            .load(\"complete.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15ef6ecd-cc28-49ac-8faf-84f4fdafc566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Name of State / UT: string (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- Longitude: string (nullable = true)\n",
      " |-- Total Confirmed cases: string (nullable = true)\n",
      " |-- Death: string (nullable = true)\n",
      " |-- Cured/Discharged/Migrated: string (nullable = true)\n",
      " |-- New cases: string (nullable = true)\n",
      " |-- New deaths: string (nullable = true)\n",
      " |-- New recovered: string (nullable = true)\n",
      " |-- total_case: long (nullable = true)\n",
      " |-- total_newly_recovered: long (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- death_Case: long (nullable = true)\n",
      " |-- new_cases: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import types\n",
    "df = df.withColumn(\"total_case\", df[\"Total Confirmed cases\"].cast(types.LongType()))\n",
    "df = df.withColumn(\"total_newly_recovered\", df[\"New recovered\"].cast(types.LongType()))\n",
    "df = df.withColumn(\"new_cases\", df[\"New cases\"].cast(types.LongType()))\n",
    "df = df.withColumn(\"state\", df[\"Name of State / UT\"].cast(types.StringType()))\n",
    "df = df.withColumn(\"death_Case\", df[\"Death\"].cast(types.LongType()))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61312be4-38ce-4b08-9511-a7f7dc93abed",
   "metadata": {},
   "source": [
    "## 1. Convert All State Names to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f798f5e7-6633-4b2d-a232-a523b7afad6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|         state_lower|\n",
      "+--------------------+\n",
      "|               delhi|\n",
      "|         maharashtra|\n",
      "|           meghalaya|\n",
      "|              odisha|\n",
      "|             haryana|\n",
      "|         west bengal|\n",
      "|                 goa|\n",
      "|              punjab|\n",
      "|   jammu and kashmir|\n",
      "|dadra and nagar h...|\n",
      "|           karnataka|\n",
      "|      andhra pradesh|\n",
      "|           telangana|\n",
      "|            nagaland|\n",
      "|               bihar|\n",
      "|      madhya pradesh|\n",
      "|           jharkhand|\n",
      "|               assam|\n",
      "|              kerala|\n",
      "|          tamil nadu|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lower, col\n",
    "\n",
    "output_df_1 = df.withColumn('state_lower', lower(col(\"state\")))\n",
    "output_df_1.select(\"state_lower\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1390ab3c-fece-4796-b872-19d60444b134",
   "metadata": {},
   "source": [
    "## 2. Find the Day with the Greatest Number of COVID Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f7224d4f-ba79-4ddc-bc07-3b819cb16e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+\n",
      "|      Date|sum(total_case)|\n",
      "+----------+---------------+\n",
      "|2020-08-06|        1964536|\n",
      "+----------+---------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output_df_2 = df.groupBy(\"Date\").sum(\"total_case\").orderBy(\"sum(total_case)\", ascending=False)\n",
    "output_df_2.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7166525-5bc5-4f95-9402-adfa7b51c2d8",
   "metadata": {},
   "source": [
    "## 3. Find the State with the Second-Largest Number of COVID Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1db0fca4-f677-4030-9ac4-f2bed5698e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(State='Tamil Nadu', sum(total_case)=7847083)\n"
     ]
    }
   ],
   "source": [
    "df_grouped_by_state = df.groupBy(\"State\").sum(\"total_case\").orderBy(\"sum(total_case)\", ascending=False)\n",
    "\n",
    "second_largest_state = df_grouped_by_state.collect()[1]  # Get the second row\n",
    "print(second_largest_state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2492a3-3c62-4c71-a481-245ce38fa45e",
   "metadata": {},
   "source": [
    "## 4. Find the Union Territory with the Least Number of Deaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d16bb3d0-d104-4a1e-8161-8a54a336f0ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+\n",
      "|               state|sum(death_Case)|\n",
      "+--------------------+---------------+\n",
      "|Union Territory o...|              0|\n",
      "+--------------------+---------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_territories = df.filter(df[\"state\"].contains(\"Union Territory\"))\n",
    "\n",
    "df_least_deaths = df_territories.groupBy(\"state\").sum(\"death_Case\").orderBy(\"sum(death_Case)\").show(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5dee28-fc00-4044-a733-58a383a31478",
   "metadata": {},
   "source": [
    "## 5. Find the State with the Lowest Death to Total Confirmed Cases Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12c74a88-332d-49a2-b89e-dcb7476dfaaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|     state|death_ratio|\n",
      "+----------+-----------+\n",
      "|Puducherry|       NULL|\n",
      "+----------+-----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_ratio = df.withColumn(\"death_ratio\", col(\"death_Case\") / col(\"total_case\"))\n",
    "\n",
    "df_lowest_ratio = df_ratio.orderBy(\"death_ratio\").select(\"state\", \"death_ratio\").show(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6557c8d1-c271-41e8-9466-c84488664aa5",
   "metadata": {},
   "source": [
    "## 6. Find the Month with the Most Newer Recovered Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dabf5973-4b27-4959-af74-165029e5c35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month with the highest number of newly recovered cases: July\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import month, col\n",
    "\n",
    "# Extract month from the \"Date\" column\n",
    "df_with_month = df.withColumn(\"month\", month(\"Date\"))\n",
    "\n",
    "# Group by month and sum the newly recovered cases\n",
    "df_grouped_by_month = df_with_month.groupBy(\"month\").sum(\"total_newly_recovered\")\n",
    "\n",
    "# Order by the sum of newly recovered cases in descending order and select the top month\n",
    "df_grouped_by_month = df_grouped_by_month.orderBy(col(\"sum(total_newly_recovered)\").desc())\n",
    "top_month = df_grouped_by_month.first()  # Get the top row\n",
    "\n",
    "# Convert month number to month name\n",
    "month_dict = {\n",
    "    1: \"January\", 2: \"February\", 3: \"March\", 4: \"April\", 5: \"May\", 6: \"June\",\n",
    "    7: \"July\", 8: \"August\", 9: \"September\", 10: \"October\", 11: \"November\", 12: \"December\"\n",
    "}\n",
    "\n",
    "if top_month:\n",
    "    top_month_number = top_month[\"month\"]\n",
    "    top_month_name = month_dict.get(top_month_number, \"Unknown\")\n",
    "    print(f\"Month with the highest number of newly recovered cases: {top_month_name}\")\n",
    "else:\n",
    "    print(\"No data available.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10066dc7-4b05-48e0-bae3-3c1e99fdefe5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
