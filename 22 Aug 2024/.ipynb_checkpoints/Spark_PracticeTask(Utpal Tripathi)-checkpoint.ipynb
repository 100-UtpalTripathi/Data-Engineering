{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8f6eb4a-4e75-420f-897d-7c8abcc8b06f",
   "metadata": {},
   "source": [
    "## Name - Utpal Tripathi                     Mail - utripathi@presidio.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a21bad6-f3bf-453f-a7ae-7d6209e20bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark  # Import findspark to locate the Spark installation\n",
    "findspark.init()  # Initialize findspark to set up the environment for Spark\n",
    "\n",
    "from pyspark.sql import SparkSession  # Import SparkSession from pyspark.sql\n",
    "\n",
    "# Initialize a Spark session\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Covid Data Analysis\") \\\n",
    "    .getOrCreate()  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1a743a1-32af-4cb8-bcbd-6f37952d73e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file into a DataFrame\n",
    "df = spark.read.format(\"csv\") \\\n",
    "            .option(\"header\", True) \\\n",
    "            .option(\"multiLine\", True) \\\n",
    "            .option(\"ignoreLeadingWhiteSpace\",True) \\\n",
    "            .option(\"ignoreTrailingWhiteSpace\",True) \\\n",
    "            .option(\"escape\", \"\\\\\") \\\n",
    "            .option(\"quote\", \"\\\"\") \\\n",
    "            .load(\"complete.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15ef6ecd-cc28-49ac-8faf-84f4fdafc566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Name of State / UT: string (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- Longitude: string (nullable = true)\n",
      " |-- Total Confirmed cases: string (nullable = true)\n",
      " |-- Death: string (nullable = true)\n",
      " |-- Cured/Discharged/Migrated: string (nullable = true)\n",
      " |-- New cases: string (nullable = true)\n",
      " |-- New deaths: string (nullable = true)\n",
      " |-- New recovered: string (nullable = true)\n",
      " |-- total_case: long (nullable = true)\n",
      " |-- total_newly_recovered: long (nullable = true)\n",
      " |-- new_cases: long (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- death_Case: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import types\n",
    "\n",
    "# Cast columns to appropriate data types\n",
    "df = df.withColumn(\"total_case\", df[\"Total Confirmed cases\"].cast(types.LongType()))  # Cast 'Total Confirmed cases' to LongType for integer values\n",
    "df = df.withColumn(\"total_newly_recovered\", df[\"New recovered\"].cast(types.LongType()))  # Cast 'New recovered' to LongType for integer values\n",
    "df = df.withColumn(\"new_cases\", df[\"New cases\"].cast(types.LongType()))  # Cast 'New cases' to LongType for integer values\n",
    "df = df.withColumn(\"state\", df[\"Name of State / UT\"].cast(types.StringType()))  # Cast 'Name of State / UT' to StringType for textual data\n",
    "df = df.withColumn(\"death_Case\", df[\"Death\"].cast(types.LongType()))  # Cast 'Death' to LongType for integer values\n",
    "\n",
    "# Print the schema of the DataFrame to verify the changes\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61312be4-38ce-4b08-9511-a7f7dc93abed",
   "metadata": {},
   "source": [
    "## 1. Convert All State Names to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f798f5e7-6633-4b2d-a232-a523b7afad6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|         state_lower|\n",
      "+--------------------+\n",
      "|               delhi|\n",
      "|         maharashtra|\n",
      "|           meghalaya|\n",
      "|              odisha|\n",
      "|             haryana|\n",
      "|         west bengal|\n",
      "|                 goa|\n",
      "|              punjab|\n",
      "|   jammu and kashmir|\n",
      "|dadra and nagar h...|\n",
      "|           karnataka|\n",
      "|      andhra pradesh|\n",
      "|           telangana|\n",
      "|            nagaland|\n",
      "|               bihar|\n",
      "|      madhya pradesh|\n",
      "|           jharkhand|\n",
      "|               assam|\n",
      "|              kerala|\n",
      "|          tamil nadu|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lower, col\n",
    "\n",
    "# Convert the 'state' column to lowercase and create a new column 'state_lower'\n",
    "output_df_1 = df.withColumn('state_lower', lower(col(\"state\")))\n",
    "\n",
    "# Select distinct values from the 'state_lower' column and display them\n",
    "output_df_1.select(\"state_lower\").distinct().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1390ab3c-fece-4796-b872-19d60444b134",
   "metadata": {},
   "source": [
    "## 2. Find the Day with the Greatest Number of COVID Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7224d4f-ba79-4ddc-bc07-3b819cb16e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+\n",
      "|      Date|sum(total_case)|\n",
      "+----------+---------------+\n",
      "|2020-08-06|        1964536|\n",
      "+----------+---------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Group by the 'Date' column and sum the 'total_case' for each date\n",
    "output_df_2 = df.groupBy(\"Date\").sum(\"total_case\")\n",
    "\n",
    "# Order the results by the summed 'total_case' in descending order\n",
    "output_df_2 = output_df_2.orderBy(\"sum(total_case)\", ascending=False)\n",
    "\n",
    "# Show the top row (the day with the greatest number of covid cases)\n",
    "output_df_2.show(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7166525-5bc5-4f95-9402-adfa7b51c2d8",
   "metadata": {},
   "source": [
    "## 3. Find the State with the Second-Largest Number of COVID Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1db0fca4-f677-4030-9ac4-f2bed5698e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(State='Tamil Nadu', sum(total_case)=7847083)\n"
     ]
    }
   ],
   "source": [
    "# Group by 'State' and sum the 'total_case' for each state\n",
    "df_grouped_by_state = df.groupBy(\"State\").sum(\"total_case\")\n",
    "\n",
    "# Order the results by the summed 'total_case' in descending order\n",
    "df_grouped_by_state = df_grouped_by_state.orderBy(\"sum(total_case)\", ascending=False)\n",
    "\n",
    "# Collect the results into a list and get the second row\n",
    "# Note: Collecting results can be expensive if the DataFrame is large.\n",
    "second_largest_state = df_grouped_by_state.collect()[1]  # Index 1 for the second row\n",
    "\n",
    "# Print the result\n",
    "print(second_largest_state)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2492a3-3c62-4c71-a481-245ce38fa45e",
   "metadata": {},
   "source": [
    "## 4. Find the Union Territory with the Least Number of Deaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d16bb3d0-d104-4a1e-8161-8a54a336f0ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+\n",
      "|               state|sum(death_Case)|\n",
      "+--------------------+---------------+\n",
      "|Union Territory o...|              0|\n",
      "+--------------------+---------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Filter rows where the 'state' column contains \"Union Territory\"\n",
    "df_territories = df.filter(col(\"state\").contains(\"Union Territory\"))\n",
    "\n",
    "# Group by 'state', sum the 'death_Case', and order the results to find the Union Territory with the least number of deaths\n",
    "df_least_deaths = df_territories.groupBy(\"state\").sum(\"death_Case\") \\\n",
    "                                .orderBy(\"sum(death_Case)\")  # Order by total deaths in ascending order\n",
    "\n",
    "# Show the result\n",
    "df_least_deaths.show(1)  # Display the top row with the least number of deaths\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5dee28-fc00-4044-a733-58a383a31478",
   "metadata": {},
   "source": [
    "## 5. Find the State with the Lowest Death to Total Confirmed Cases Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "12c74a88-332d-49a2-b89e-dcb7476dfaaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+--------+---------+---------------------+-----+-------------------------+---------+----------+-------------+----------+---------------------+---------+------+----------+\n",
      "|      Date|Name of State / UT|Latitude|Longitude|Total Confirmed cases|Death|Cured/Discharged/Migrated|New cases|New deaths|New recovered|total_case|total_newly_recovered|new_cases| state|death_Case|\n",
      "+----------+------------------+--------+---------+---------------------+-----+-------------------------+---------+----------+-------------+----------+---------------------+---------+------+----------+\n",
      "|2020-01-30|            Kerala| 10.8505|  76.2711|                  1.0|    0|                      0.0|        0|         0|            0|         1|                    0|        0|Kerala|         0|\n",
      "|2020-01-31|            Kerala| 10.8505|  76.2711|                  1.0|    0|                      0.0|        0|         0|            0|         1|                    0|        0|Kerala|         0|\n",
      "|2020-02-01|            Kerala| 10.8505|  76.2711|                  2.0|    0|                      0.0|        1|         0|            0|         2|                    0|        1|Kerala|         0|\n",
      "|2020-02-02|            Kerala| 10.8505|  76.2711|                  3.0|    0|                      0.0|        1|         0|            0|         3|                    0|        1|Kerala|         0|\n",
      "|2020-02-03|            Kerala| 10.8505|  76.2711|                  3.0|    0|                      0.0|        0|         0|            0|         3|                    0|        0|Kerala|         0|\n",
      "|2020-02-04|            Kerala| 10.8505|  76.2711|                  3.0|    0|                      0.0|        0|         0|            0|         3|                    0|        0|Kerala|         0|\n",
      "|2020-02-05|            Kerala| 10.8505|  76.2711|                  3.0|    0|                      0.0|        0|         0|            0|         3|                    0|        0|Kerala|         0|\n",
      "|2020-02-06|            Kerala| 10.8505|  76.2711|                  3.0|    0|                      0.0|        0|         0|            0|         3|                    0|        0|Kerala|         0|\n",
      "|2020-02-07|            Kerala| 10.8505|  76.2711|                  3.0|    0|                      0.0|        0|         0|            0|         3|                    0|        0|Kerala|         0|\n",
      "|2020-02-08|            Kerala| 10.8505|  76.2711|                  3.0|    0|                      0.0|        0|         0|            0|         3|                    0|        0|Kerala|         0|\n",
      "|2020-02-09|            Kerala| 10.8505|  76.2711|                  3.0|    0|                      0.0|        0|         0|            0|         3|                    0|        0|Kerala|         0|\n",
      "|2020-02-10|            Kerala| 10.8505|  76.2711|                  3.0|    0|                      0.0|        0|         0|            0|         3|                    0|        0|Kerala|         0|\n",
      "|2020-02-11|            Kerala| 10.8505|  76.2711|                  3.0|    0|                      0.0|        0|         0|            0|         3|                    0|        0|Kerala|         0|\n",
      "|2020-02-12|            Kerala| 10.8505|  76.2711|                  3.0|    0|                      0.0|        0|         0|            0|         3|                    0|        0|Kerala|         0|\n",
      "|2020-02-13|            Kerala| 10.8505|  76.2711|                  3.0|    0|                      0.0|        0|         0|            0|         3|                    0|        0|Kerala|         0|\n",
      "|2020-02-14|            Kerala| 10.8505|  76.2711|                  3.0|    0|                      0.0|        0|         0|            0|         3|                    0|        0|Kerala|         0|\n",
      "|2020-02-15|            Kerala| 10.8505|  76.2711|                  3.0|    0|                      0.0|        0|         0|            0|         3|                    0|        0|Kerala|         0|\n",
      "|2020-02-16|            Kerala| 10.8505|  76.2711|                  3.0|    0|                      0.0|        0|         0|            0|         3|                    0|        0|Kerala|         0|\n",
      "|2020-02-17|            Kerala| 10.8505|  76.2711|                  3.0|    0|                      0.0|        0|         0|            0|         3|                    0|        0|Kerala|         0|\n",
      "|2020-02-18|            Kerala| 10.8505|  76.2711|                  3.0|    0|                      0.0|        0|         0|            0|         3|                    0|        0|Kerala|         0|\n",
      "+----------+------------------+--------+---------+---------------------+-----+-------------------------+---------+----------+-------------+----------+---------------------+---------+------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----------+-----------+\n",
      "|     state|death_ratio|\n",
      "+----------+-----------+\n",
      "|Puducherry|       NULL|\n",
      "+----------+-----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()\n",
    "# Calculate the death-to-case ratio for each row\n",
    "df_ratio = df.withColumn(\"death_ratio\", col(\"death_Case\") / col(\"total_case\"))\n",
    "\n",
    "# Find the state with the lowest death-to-case ratio by ordering the DataFrame by 'death_ratio' in ascending order\n",
    "df_lowest_ratio = df_ratio.orderBy(\"death_ratio\").select(\"state\", \"death_ratio\")\n",
    "\n",
    "# Display the top row which has the lowest ratio\n",
    "df_lowest_ratio.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6557c8d1-c271-41e8-9466-c84488664aa5",
   "metadata": {},
   "source": [
    "## 6. Find the Month with the Most Newer Recovered Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dabf5973-4b27-4959-af74-165029e5c35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month with the highest number of newly recovered cases: July (722983 cases)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import month\n",
    "\n",
    "# Extract month from the \"Date\" column\n",
    "df_with_month = df.withColumn(\"month\", month(\"Date\"))\n",
    "\n",
    "# Group by month and sum the newly recovered cases\n",
    "df_grouped_by_month = df_with_month.groupBy(\"month\").sum(\"total_newly_recovered\")\n",
    "\n",
    "# Order by the sum of newly recovered cases in descending order and select the top month\n",
    "df_grouped_by_month = df_grouped_by_month.orderBy(col(\"sum(total_newly_recovered)\").desc())\n",
    "top_month = df_grouped_by_month.first()  # Get the top row\n",
    "\n",
    "# Convert month number to month name\n",
    "month_dict = {\n",
    "    1: \"January\", 2: \"February\", 3: \"March\", 4: \"April\", 5: \"May\", 6: \"June\",\n",
    "    7: \"July\", 8: \"August\", 9: \"September\", 10: \"October\", 11: \"November\", 12: \"December\"\n",
    "}\n",
    "\n",
    "if top_month:\n",
    "    top_month_number = top_month[\"month\"]\n",
    "    top_month_name = month_dict.get(top_month_number, \"Unknown\")\n",
    "    top_month_cases = top_month[\"sum(total_newly_recovered)\"]\n",
    "    print(f\"Month with the highest number of newly recovered cases: {top_month_name} ({top_month_cases} cases)\")\n",
    "else:\n",
    "    print(\"No data available.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee377bc-6884-420e-b6e0-1a0ec2f7a780",
   "metadata": {},
   "source": [
    "## ThankYou..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22adf8dd-e8c1-4904-bb7d-c1817b843cf7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
